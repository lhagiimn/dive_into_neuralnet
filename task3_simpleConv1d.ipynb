{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "    y : ndarray, shape (n_samples, 1)\n",
    "    batch_size : int\n",
    "    seed : int\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "    n_nodes2 : int\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.W_feedback = 0\n",
    "        self.B_feedback = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (batch_size, n_nodes1)\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size, n_nodes2)\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \n",
    "        \n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        self.W_feedback = self.dW / self.dA.shape[0]\n",
    "        self.B_feedback = np.average(self.dA, axis=0)\n",
    "        \n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    " \n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return (1 - self._func(X)) * self._func(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "       \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return 1 - (self._func(X))**2\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "        self.pred = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        X = X - np.max(X)\n",
    "        tmp = np.exp(X)\n",
    "        denominator = np.sum(tmp, axis=1)\n",
    "        output = tmp / denominator[:, np.newaxis]\n",
    "        return output\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return X\n",
    "        \n",
    "    def forward(self, X):\n",
    "  \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        self.pred = A\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "\n",
    "        dZ = self.pred - dA\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "  \n",
    "    def __init__(self, sigma = 0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return np.where( x > 0, 1, 0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "       \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) / np.sqrt(self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) * np.sqrt(2 / self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    " \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.H_B = 1\n",
    "        self.H_W = 1\n",
    "    def update(self, layer):\n",
    "            \n",
    "        #dA, dWを更新＆保存\n",
    "        self.H_B = self.H_B + np.average(layer.dA)**2\n",
    "        self.H_W = self.H_W + np.average(layer.dW)**2\n",
    "        \n",
    "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0) / np.sqrt(self.H_B)\n",
    "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0] / np.sqrt(self.H_W)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        layer.B = layer.B - self.lr * layer.B_feedback    \n",
    "        layer.W = layer.W - self.lr * layer.W_feedback\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "\n",
    "    def __init__(self, n_input_hight, f_w, f_b, optimizer):\n",
    "        DIM = 1\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.f_hight = len(f_w)\n",
    "        self.n_input_hight = n_input_hight\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w[:, np.newaxis]\n",
    "        self.B = f_b[:, np.newaxis]\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dB = 0\n",
    "        print(\"N_input:{} F_hight:{}\".format(self.n_input_hight, self.f_hight))\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        #self.n_output_width = self.n_input_width - f_width +1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros([self.n_output_hight, DIM])\n",
    "        self.W_feedback = np.zeros([self.f_hight, DIM])\n",
    "        self.B_feedback = 0\n",
    "        self.Z_feedback = np.zeros([self.n_input_hight, DIM])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            \n",
    "            X_seg = X[h1:h2]\n",
    "            self.output_X_forward[h] = np.dot(X_seg, self.W) + self.B\n",
    "        \n",
    "        return self.output_X_forward\n",
    "    \n",
    "    def backward(self, dA):\n",
    "     \n",
    "        dA = dA[:,np.newaxis]\n",
    "        for i in range(self.f_hight):\n",
    "            X_seg = self.input_X_forward[i : (i + self.n_output_hight)]\n",
    "            X_seg = X_seg[:,np.newaxis]\n",
    "            self.W_feedback[i] = np.dot(X_seg.T, dA)\n",
    "        \n",
    "\n",
    "        self.B_feedback = np.sum(dA, axis=0)\n",
    "\n",
    "        dA_padding = np.zeros([self.f_hight-1, 1])\n",
    "        dA = np.concatenate((dA, dA_padding), axis=0)\n",
    "        dA = np.concatenate((dA_padding, dA), axis=0)\n",
    "        for h in range(self.n_input_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            dA_seg = dA[h1:h2]\n",
    "\n",
    "            dA_seg = np.fliplr(dA_seg.T).T\n",
    "            self.Z_feedback[h] = np.dot(dA_seg.T, self.W)\n",
    "\n",
    "\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_output_size(n_feature_in, n_pading, n_filter, stride):\n",
    "    return (n_feature_in + n_pading * 2 + n_filter) / stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#initializer = SimpleInitializer()\n",
    "optimizer = SGD(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_input:4 F_hight:3\n"
     ]
    }
   ],
   "source": [
    "scv = SimpleConv1d(len(x), w, b, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35.],\n",
       "       [50.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scv.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30.],\n",
       "       [110.],\n",
       "       [170.],\n",
       "       [140.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scv.backward(delta_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_b: [30]\n",
      "delta_w: [[ 50.]\n",
      " [ 80.]\n",
      " [110.]]\n",
      "delta_x: [[ 30.]\n",
      " [110.]\n",
      " [170.]\n",
      " [140.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"delta_b:\",scv.B_feedback)\n",
    "print(\"delta_w:\",scv.W_feedback)\n",
    "print(\"delta_x:\",scv.Z_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    " \n",
    "    def __init__(self, n_input_hight, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.n_input_hight = n_input_hight\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w\n",
    "        self.B = f_b\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "            \n",
    "        self.input_X_forward = X\n",
    "        #for output_ch in range(self.W.shape[0]):\n",
    "        A = np.zeros((self.n_output_hight, self.W.shape[0]))\n",
    "        #self.input_X_forward = np.zeros((self.n_output_hight, X.shape[0], self.f_hight))\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight   \n",
    "            X_seg = X[:, h1:h2]\n",
    "            tmp = np.sum(X_seg * self.W, axis=1)\n",
    "            A[h] = np.sum(tmp, axis=1)\n",
    "\n",
    "        B = self.B[0]\n",
    "        output = (A + B).T\n",
    "        \n",
    "        #print(\"output.shape:\",output.shape)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    " \n",
    "        X = np.tile(self.input_X_forward, (dA.shape[0] ,1))\n",
    "        dL = np.zeros((X.shape[0], dA.shape[1]))\n",
    "        \n",
    "        for i in range(dA.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.input_X_forward.shape[0]\n",
    "            dL[o1:o2] = np.tile(dA[i], (self.input_X_forward.shape[0] ,1))\n",
    "        \n",
    "\n",
    "        loop = self.input_X_forward.shape[1] - dA.shape[1] + 1\n",
    "        dW_tmp = np.zeros((X.shape[0], loop))\n",
    "        for i in range(loop):\n",
    "            i1 = i\n",
    "            i2 = i + dA.shape[1]\n",
    "            dX_seg = X[:, i1:i2]\n",
    "            dW_tmp[:,i] = np.sum(dL * dX_seg, axis=1)\n",
    "        \n",
    "\n",
    "        for i in range(self.W.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.W.shape[1]\n",
    "            self.W_feedback[i] = dW_tmp[o1:o2]\n",
    "        \n",
    "        dB = np.sum(dA, axis=1)\n",
    "        for i in range(self.B.shape[1]):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(dA.shape[0]):\n",
    "\n",
    "            dA_padding = np.zeros([1, self.f_hight-1])\n",
    "            dA_tmp = dA[i][np.newaxis,:]\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=1)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=1)\n",
    "            dA_tmp = np.tile(dA_tmp, (self.input_X_forward.shape[0] ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                dA_seg = dA_tmp[:,h1:h2]\n",
    "                dA_seg = np.fliplr(dA_seg.T).T\n",
    "                dZ_seg[:,h] = np.sum(dA_seg * self.W[i], axis=1)\n",
    "                \n",
    "            self.Z_feedback += dZ_seg \n",
    "            \n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3,4],[2,3,4,5]])\n",
    "W = np.ones((3,2,3))\n",
    "B = np.array([[[1,2,3], [1,2,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "c = Conv1d(X.shape[1], W, B, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = np.array([[2,4],[4,8],[6,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 36, 36, 24],\n",
       "       [12, 36, 36, 24]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.backward(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.loss = 0\n",
    "        self.loss_val = 0\n",
    "        self.activation_func = 0\n",
    "        self.affine_func = 0\n",
    "        self.n_layer = 0\n",
    "        self.layer_instance = [0 for _ in range(64)]\n",
    "  \n",
    "\n",
    "    def _crossentropy(self, y_pred, y):\n",
    "\n",
    "        INF_AVOIDANCE = 1e-8\n",
    "        cross_entropy = -1 * y * np.log(y_pred + INF_AVOIDANCE)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def add_layer(self, model):\n",
    "        self.layer_instance[self.n_layer] = model\n",
    "        self.n_layer += 1\n",
    "        return\n",
    "    \n",
    "    def delet_all_layer(self):\n",
    "        self.layer_instance[0:self.n_layer] = 0\n",
    "        self.n_layer = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        self.loss_val = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                #Forwardの計算\n",
    "                for layer in range(self.n_layer):\n",
    "                    X = self.layer_instance[layer].forward(X)\n",
    "                \n",
    "        \n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                    \n",
    "   \n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    dz = self.layer_instance[layer].backward(dz)\n",
    "                \n",
    "                loop_count += 1\n",
    "                \n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self._predict(X_val)\n",
    "                self.loss_val[i] = self._crossentropy(y_val_pred, y_val)\n",
    "                \n",
    "            if self.verbose:\n",
    "                print(\"Epoch:{} Loss:{} Loss(val):{}\".format(i, self.loss[i], self.loss_val[i]))\n",
    "                \n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "   \n",
    "        for layer in range(self.n_layer):\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "\n",
    "    def _predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer): \n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_X_shape = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.inout_X_shape = X.shape\n",
    "        return X.reshape(-1)[np.newaxis,:]\n",
    "    \n",
    "    def backward(self, X):\n",
    "        output = X.reshape(self.inout_X_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 784)\n",
      "(57000, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15022/1027773539.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_train = x_train.astype(np.float)\n",
      "/tmp/ipykernel_15022/1027773539.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_test = x_test.astype(np.float)\n",
      "/home/lkhagvadorj/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.95)\n",
    "print(x_train.shape) # (48000, 784)\n",
    "print(x_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN = ScratchDeepNeuralNetrowkClassifier(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w = np.ones((3,1,28))\n",
    "f_b = np.array([[[1,1,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(0.01)\n",
    "initializer = XavierInitializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN.add_layer(Conv1d(x_train.shape[1], f_w, f_b, initializer, optimizer))\n",
    "CNN.add_layer(Flatten())\n",
    "CNN.add_layer(FC(f_w.shape[0] * (x_train.shape[1] - f_w.shape[2] + 1), 100, initializer, optimizer))\n",
    "CNN.add_layer(Sigmoid())\n",
    "CNN.add_layer(FC(100, 10, initializer, optimizer))\n",
    "CNN.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15022/185089456.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "/tmp/ipykernel_15022/3289879148.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-1 * X))\n"
     ]
    }
   ],
   "source": [
    "CNN.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15022/3289879148.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-1 * X))\n"
     ]
    }
   ],
   "source": [
    "y_pred = CNN.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred=\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "Yval=\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred=\\n\", y_pred)\n",
    "print(\"Yval=\\n\", y_val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score=0.800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred[0], y_val[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "\n",
    "    def __init__(self, n_input_hight, padding_size, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.padding_size = padding_size\n",
    "        self.n_input_hight = n_input_hight + self.padding_size * 2\n",
    "\n",
    "        self.W = f_w\n",
    "        self.B = f_b\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "   \n",
    "        \n",
    "        #Padding\n",
    "        X = np.pad(X, (self.padding_size, self.padding_size), 'constant', constant_values=(0, 0))\n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        A = np.zeros((self.n_output_hight, self.W.shape[0]))\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight   \n",
    "            X_seg = X[:, h1:h2]\n",
    "            tmp = np.sum(X_seg * self.W, axis=1)\n",
    "            A[h] = np.sum(tmp, axis=1)\n",
    "\n",
    "        B = self.B[0]\n",
    "        output = (A + B).T\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[0] ,1))\n",
    "        dL = np.zeros((X.shape[0], dA.shape[1]))\n",
    "        \n",
    "        for i in range(dA.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.input_X_forward.shape[0]\n",
    "            dL[o1:o2] = np.tile(dA[i], (self.input_X_forward.shape[0] ,1))\n",
    "        \n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop = self.input_X_forward.shape[1] - dA.shape[1] + 1\n",
    "        dW_tmp = np.zeros((X.shape[0], loop))\n",
    "        for i in range(loop):\n",
    "            i1 = i\n",
    "            i2 = i + dA.shape[1]\n",
    "            dX_seg = X[:, i1:i2]\n",
    "            dW_tmp[:,i] = np.sum(dL * dX_seg, axis=1)\n",
    "    \n",
    "        for i in range(self.W.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.W.shape[1]\n",
    "            self.W_feedback[i] = dW_tmp[o1:o2]\n",
    "        \n",
    "\n",
    "        dB = np.sum(dA, axis=1)\n",
    "        for i in range(self.B.shape[1]):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(dA.shape[0]):\n",
    "\n",
    "            dA_padding = np.zeros([1, self.f_hight-1])\n",
    "            dA_tmp = dA[i][np.newaxis,:]\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=1)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=1)\n",
    "            dA_tmp = np.tile(dA_tmp, (self.input_X_forward.shape[0] ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                dA_seg = dA_tmp[:,h1:h2]\n",
    "      \n",
    "                dA_seg = np.fliplr(dA_seg.T).T\n",
    "                dZ_seg[:,h] = np.sum(dA_seg * self.W[i], axis=1)\n",
    "                \n",
    "            self.Z_feedback += dZ_seg \n",
    "\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1, 784)\n",
      "(57000, 1, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15022/1041005328.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_train = x_train.astype(np.float)\n",
      "/tmp/ipykernel_15022/1041005328.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_test = x_test.astype(np.float)\n",
      "/home/lkhagvadorj/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "\n",
    "x_train = x_train[:,np.newaxis,:]\n",
    "x_test = x_test[:,np.newaxis,:]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.95)\n",
    "print(x_train.shape) \n",
    "print(x_val.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15022/185089456.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "get_mini_batch = GetMiniBatch(x_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_X_train, mini_y_train = get_mini_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 784)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_X_shape = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, n_input, n_feature1)\n",
    "        \n",
    "        return (batch_size, n_input * n_feature)\n",
    "        \"\"\"\n",
    "        self.inout_X_shape = X.shape\n",
    "        output = X.reshape([self.inout_X_shape[0], self.inout_X_shape[1] * self.inout_X_shape[2]])\n",
    "        return output\n",
    "    \n",
    "    def backward(self, X):\n",
    "        output = X.reshape(self.inout_X_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[[1,2],[2,3]],[[4,5],[6,7]],[[8,9],[10,11]]])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2():\n",
    "\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.loss = 0\n",
    "        self.loss_val = 0\n",
    "        self.activation_func = 0\n",
    "        self.affine_func = 0\n",
    "        self.n_layer = 0\n",
    "        self.layer_instance = [0 for _ in range(64)]\n",
    "\n",
    "        \n",
    "    def _crossentropy(self, y_pred, y):\n",
    "        INF_AVOIDANCE = 1e-8\n",
    "        cross_entropy = -1 * y * np.log(y_pred + INF_AVOIDANCE)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def add_layer(self, model):\n",
    "        self.layer_instance[self.n_layer] = model\n",
    "        self.n_layer += 1\n",
    "        return\n",
    "    \n",
    "    def delet_all_layer(self):\n",
    "        self.layer_instance[0:self.n_layer] = 0\n",
    "        self.n_layer = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        self.loss_val = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                for layer in range(self.n_layer):\n",
    "                    X = self.layer_instance[layer].forward(X)\n",
    "              \n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                    \n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    dz = self.layer_instance[layer].backward(dz)\n",
    "        \n",
    "                loop_count += 1\n",
    "                \n",
    "          \n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self._predict(X_val)\n",
    "                self.loss_val[i] = self._crossentropy(y_val_pred, y_val)\n",
    "                \n",
    "            if self.verbose:\n",
    "                print(\"Epoch:{} Loss:{} Loss(val):{}\".format(i, self.loss[i], self.loss_val[i]))\n",
    "                \n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        for layer in range(self.n_layer):\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "\n",
    "    def _predict(self, X):\n",
    "        for layer in range(self.n_layer):\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC2:\n",
    "  \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.W_feedback = 0\n",
    "        self.B_feedback = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "           \n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        self.W_feedback = self.dW / self.dA.shape[0]\n",
    "        self.B_feedback = np.average(self.dA, axis=0)\n",
    "\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "\n",
    "    def __init__(self, n_input_hight, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "     \n",
    "        self.n_input_hight = n_input_hight\n",
    "        self.W = f_w    #(n_output, n_ch, f_size)\n",
    "        self.B = f_b    #(1, n_ch, n_output)\n",
    "        self.n_output = self.W.shape[0]\n",
    "        self.n_input_ch = self.W.shape[1]\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "           \n",
    "        self.input_X_forward = X\n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        A = np.zeros((batch_size, self.n_output, self.n_input_ch, self.n_output_hight))\n",
    "        B = self.B[0]\n",
    "        B = B.T\n",
    "        B = B[np.newaxis]\n",
    "        X = X[:,np.newaxis]\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            X_seg = X[:,:,:,h1:h2]\n",
    "            tmp = np.sum(X_seg * self.W, axis=3)\n",
    "            tmp = tmp + B\n",
    "            A[:,:,:,h] = tmp\n",
    "\n",
    "        A = np.sum(A, axis=2)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "           \n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[1] ,1))\n",
    "        dL = np.zeros((dA.shape[0], X.shape[1], dA.shape[2]))\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i\n",
    "            o2 = i + self.n_input_ch\n",
    "            tmp = dA[:,i][:,np.newaxis,:]\n",
    "            dL[:,o1:o2] = np.tile(tmp, (self.n_input_ch ,1))\n",
    "        \n",
    "        loop = self.n_input_hight - self.n_output_hight + 1\n",
    "        dW_tmp = np.zeros((batch_size, self.n_output, loop))\n",
    "        for i in range(loop):\n",
    "            i1 = i\n",
    "            i2 = i + self.n_output_hight\n",
    "            dX_seg = X[:,:, i1:i2]\n",
    "            dW_tmp[:,:,i] = np.sum(dL * dX_seg, axis=2)\n",
    "        \n",
    "        dW_tmp2 = np.average(dW_tmp, axis=0)     \n",
    "        for i in range(dW_tmp2.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.n_input_ch\n",
    "            self.W_feedback[i] = dW_tmp2[o1:o2]\n",
    "\n",
    "\n",
    "        dB = np.sum(dA, axis=2)\n",
    "        dB = np.average(dB, axis=0) \n",
    "        for i in range(self.B.shape[1]):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(self.n_output):\n",
    "            dA_padding = np.zeros([batch_size, 1, self.f_hight-1])\n",
    "            dA_tmp = dA[:,i][:,np.newaxis,:]\n",
    "            #print(\"dA_tmp.shape1:\",dA_tmp.shape)\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=2)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=2)\n",
    "            #print(\"dA_tmp.shape2:\",dA_tmp.shape)\n",
    "            dA_tmp = np.tile(dA_tmp, (self.n_input_ch ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                dA_seg = dA_tmp[:,:,h1:h2]\n",
    "                dA_seg = np.fliplr(dA_seg.T).T\n",
    "                dZ_seg[:,:,h] = np.sum(dA_seg * self.W[i], axis=2)\n",
    "                \n",
    "            self.Z_feedback += dZ_seg \n",
    "\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w = np.ones((1,1,28))\n",
    "f_b = np.array([[[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN2 = Conv1d(x_train.shape[2], f_w, f_b, initializer, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "A = CNN2.forward(mini_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 757)\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN2.backward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN2 = ScratchDeepNeuralNetrowkClassifier2(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN2.add_layer(Conv1d(x_train.shape[2], f_w, f_b, initializer, optimizer))\n",
    "CNN2.add_layer(Flatten())\n",
    "CNN2.add_layer(FC2(f_w.shape[0] * (x_train.shape[2] - f_w.shape[2] + 1), 100, initializer, optimizer))\n",
    "CNN2.add_layer(Sigmoid())\n",
    "CNN2.add_layer(FC2(100, 10, initializer, optimizer))\n",
    "CNN2.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15022/185089456.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "CNN2.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CNN2.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred=\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Yval=\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred=\\n\", y_pred)\n",
    "print(\"Yval=\\n\", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score=0.716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = np.array(CNN2.loss)\n",
    "loss_ave = np.average(loss, axis=1)\n",
    "\n",
    "loss_val = np.array(CNN2.loss_val)\n",
    "loss_val_ave = np.average(loss_val, axis=1)\n",
    "\n",
    "plt.title(\"epoch vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ave, \"r\", label=\"loss\")\n",
    "plt.plot(loss_val_ave, \"b\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 757) (10, 100)\n"
     ]
    }
   ],
   "source": [
    "A = np.zeros([10,757])\n",
    "B = np.zeros([10,100])\n",
    "print(A.shape, B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(757, 100)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.dot(A.T, B)\n",
    "C.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
