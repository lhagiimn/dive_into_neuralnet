{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45258/3331397441.py:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)[:, np.newaxis]\n",
      "/tmp/ipykernel_45258/3331397441.py:52: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "/tmp/ipykernel_45258/3331397441.py:127: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 12.3026, val_loss : 27.5008, acc : 0.750, val_acc : 0.375\n",
      "Epoch 1, loss : 1.6502, val_loss : 1.6921, acc : 0.750, val_acc : 0.688\n",
      "Epoch 2, loss : 9.8394, val_loss : 6.5995, acc : 0.250, val_acc : 0.625\n",
      "Epoch 3, loss : 0.0179, val_loss : 1.8780, acc : 1.000, val_acc : 0.688\n",
      "Epoch 4, loss : 0.0000, val_loss : 1.2362, acc : 1.000, val_acc : 0.812\n",
      "Epoch 5, loss : 0.0000, val_loss : 0.0908, acc : 1.000, val_acc : 0.938\n",
      "Epoch 6, loss : 0.0008, val_loss : 3.0719, acc : 1.000, val_acc : 0.750\n",
      "Epoch 7, loss : 0.0000, val_loss : 0.0342, acc : 1.000, val_acc : 1.000\n",
      "Epoch 8, loss : 0.0000, val_loss : 1.6674, acc : 1.000, val_acc : 0.812\n",
      "Epoch 9, loss : 0.0000, val_loss : 0.6257, acc : 1.000, val_acc : 0.812\n",
      "Epoch 10, loss : 0.0001, val_loss : 2.3368, acc : 1.000, val_acc : 0.750\n",
      "Epoch 11, loss : 0.0000, val_loss : 0.0006, acc : 1.000, val_acc : 1.000\n",
      "Epoch 12, loss : 0.0000, val_loss : 0.0092, acc : 1.000, val_acc : 1.000\n",
      "Epoch 13, loss : 0.0001, val_loss : 2.3725, acc : 1.000, val_acc : 0.750\n",
      "Epoch 14, loss : 0.0000, val_loss : 0.0841, acc : 1.000, val_acc : 0.938\n",
      "Epoch 15, loss : 0.0023, val_loss : 3.0572, acc : 1.000, val_acc : 0.750\n",
      "Epoch 16, loss : 0.0000, val_loss : 0.0005, acc : 1.000, val_acc : 1.000\n",
      "Epoch 17, loss : 0.0000, val_loss : 0.0015, acc : 1.000, val_acc : 1.000\n",
      "Epoch 18, loss : 0.0000, val_loss : 1.3252, acc : 1.000, val_acc : 0.875\n",
      "Epoch 19, loss : 0.0000, val_loss : 0.6520, acc : 1.000, val_acc : 0.812\n",
      "Epoch 20, loss : 0.0842, val_loss : 4.0393, acc : 1.000, val_acc : 0.750\n",
      "Epoch 21, loss : 0.0000, val_loss : 0.0043, acc : 1.000, val_acc : 1.000\n",
      "Epoch 22, loss : 0.0000, val_loss : 1.4280, acc : 1.000, val_acc : 0.812\n",
      "Epoch 23, loss : 0.0001, val_loss : 2.2588, acc : 1.000, val_acc : 0.875\n",
      "Epoch 24, loss : 0.0000, val_loss : 0.0101, acc : 1.000, val_acc : 1.000\n",
      "Epoch 25, loss : 0.0000, val_loss : 0.0663, acc : 1.000, val_acc : 1.000\n",
      "Epoch 26, loss : 0.0005, val_loss : 2.7042, acc : 1.000, val_acc : 0.750\n",
      "Epoch 27, loss : 0.0000, val_loss : 0.0460, acc : 1.000, val_acc : 1.000\n",
      "Epoch 28, loss : 0.0001, val_loss : 2.5112, acc : 1.000, val_acc : 0.875\n",
      "Epoch 29, loss : 0.0000, val_loss : 0.3305, acc : 1.000, val_acc : 0.875\n",
      "Epoch 30, loss : 0.0015, val_loss : 2.9264, acc : 1.000, val_acc : 0.750\n",
      "Epoch 31, loss : 0.0000, val_loss : 0.0101, acc : 1.000, val_acc : 1.000\n",
      "Epoch 32, loss : 0.0000, val_loss : 2.2458, acc : 1.000, val_acc : 0.875\n",
      "Epoch 33, loss : 0.0000, val_loss : 0.5721, acc : 1.000, val_acc : 0.812\n",
      "Epoch 34, loss : 0.0063, val_loss : 2.8900, acc : 1.000, val_acc : 0.875\n",
      "Epoch 35, loss : 0.0000, val_loss : 0.0122, acc : 1.000, val_acc : 1.000\n",
      "Epoch 36, loss : 0.0000, val_loss : 2.3420, acc : 1.000, val_acc : 0.875\n",
      "Epoch 37, loss : 0.0000, val_loss : 0.8292, acc : 1.000, val_acc : 0.812\n",
      "Epoch 38, loss : 0.1620, val_loss : 3.9167, acc : 1.000, val_acc : 0.750\n",
      "Epoch 39, loss : 0.0000, val_loss : 0.0233, acc : 1.000, val_acc : 1.000\n",
      "Epoch 40, loss : 0.0001, val_loss : 2.5606, acc : 1.000, val_acc : 0.875\n",
      "Epoch 41, loss : 0.0000, val_loss : 0.8963, acc : 1.000, val_acc : 0.812\n",
      "Epoch 42, loss : 0.0091, val_loss : 3.6152, acc : 1.000, val_acc : 0.750\n",
      "Epoch 43, loss : 0.0000, val_loss : 0.1907, acc : 1.000, val_acc : 0.938\n",
      "Epoch 44, loss : 0.0011, val_loss : 3.0055, acc : 1.000, val_acc : 0.875\n",
      "Epoch 45, loss : 0.0000, val_loss : 1.7162, acc : 1.000, val_acc : 0.812\n",
      "Epoch 46, loss : 0.6261, val_loss : 4.9801, acc : 0.750, val_acc : 0.750\n",
      "Epoch 47, loss : 0.0000, val_loss : 0.6442, acc : 1.000, val_acc : 0.812\n",
      "Epoch 48, loss : 0.0001, val_loss : 2.8483, acc : 1.000, val_acc : 0.875\n",
      "Epoch 49, loss : 0.0294, val_loss : 4.0212, acc : 1.000, val_acc : 0.625\n",
      "Epoch 50, loss : 1.1047, val_loss : 5.1808, acc : 0.750, val_acc : 0.750\n",
      "Epoch 51, loss : 0.0000, val_loss : 1.1455, acc : 1.000, val_acc : 0.812\n",
      "Epoch 52, loss : 1.0883, val_loss : 6.0766, acc : 0.750, val_acc : 0.750\n",
      "Epoch 53, loss : 0.0000, val_loss : 0.2361, acc : 1.000, val_acc : 0.938\n",
      "Epoch 54, loss : 0.0001, val_loss : 3.2671, acc : 1.000, val_acc : 0.875\n",
      "Epoch 55, loss : 0.0038, val_loss : 3.7218, acc : 1.000, val_acc : 0.750\n",
      "Epoch 56, loss : 0.0984, val_loss : 4.7442, acc : 1.000, val_acc : 0.750\n",
      "Epoch 57, loss : 2.3363, val_loss : 9.1492, acc : 0.750, val_acc : 0.500\n",
      "Epoch 58, loss : 2.1369, val_loss : 7.3601, acc : 0.750, val_acc : 0.750\n",
      "Epoch 59, loss : 5.5888, val_loss : 16.2913, acc : 0.750, val_acc : 0.438\n",
      "Epoch 60, loss : 3.5916, val_loss : 9.0174, acc : 0.750, val_acc : 0.750\n",
      "Epoch 61, loss : 10.9726, val_loss : 27.6950, acc : 0.750, val_acc : 0.375\n",
      "Epoch 62, loss : 0.0003, val_loss : 4.3574, acc : 1.000, val_acc : 0.875\n",
      "Epoch 63, loss : 0.0000, val_loss : 2.0887, acc : 1.000, val_acc : 0.812\n",
      "Epoch 64, loss : 0.0000, val_loss : 0.0001, acc : 1.000, val_acc : 1.000\n",
      "Epoch 65, loss : 0.0000, val_loss : 0.0001, acc : 1.000, val_acc : 1.000\n",
      "Epoch 66, loss : 0.0000, val_loss : 0.0014, acc : 1.000, val_acc : 1.000\n",
      "Epoch 67, loss : 0.0000, val_loss : 0.5976, acc : 1.000, val_acc : 0.938\n",
      "Epoch 68, loss : 0.0000, val_loss : 0.5694, acc : 1.000, val_acc : 0.812\n",
      "Epoch 69, loss : 0.0000, val_loss : 0.0374, acc : 1.000, val_acc : 1.000\n",
      "Epoch 70, loss : 0.0000, val_loss : 0.0001, acc : 1.000, val_acc : 1.000\n",
      "Epoch 71, loss : 0.0000, val_loss : 0.0001, acc : 1.000, val_acc : 1.000\n",
      "Epoch 72, loss : 0.0000, val_loss : 0.0699, acc : 1.000, val_acc : 0.938\n",
      "Epoch 73, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 74, loss : 0.0000, val_loss : 0.0004, acc : 1.000, val_acc : 1.000\n",
      "Epoch 75, loss : 0.0000, val_loss : 0.0223, acc : 1.000, val_acc : 1.000\n",
      "Epoch 76, loss : 0.0000, val_loss : 0.0006, acc : 1.000, val_acc : 1.000\n",
      "Epoch 77, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 78, loss : 0.0000, val_loss : 0.0001, acc : 1.000, val_acc : 1.000\n",
      "Epoch 79, loss : 0.0000, val_loss : 0.0005, acc : 1.000, val_acc : 1.000\n",
      "Epoch 80, loss : 0.0000, val_loss : 0.0014, acc : 1.000, val_acc : 1.000\n",
      "Epoch 81, loss : 0.0000, val_loss : 0.0020, acc : 1.000, val_acc : 1.000\n",
      "Epoch 82, loss : 0.0000, val_loss : 0.0019, acc : 1.000, val_acc : 1.000\n",
      "Epoch 83, loss : 0.0000, val_loss : 0.0016, acc : 1.000, val_acc : 1.000\n",
      "Epoch 84, loss : 0.0000, val_loss : 0.0013, acc : 1.000, val_acc : 1.000\n",
      "Epoch 85, loss : 0.0000, val_loss : 0.0010, acc : 1.000, val_acc : 1.000\n",
      "Epoch 86, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 87, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 88, loss : 0.0000, val_loss : 0.0008, acc : 1.000, val_acc : 1.000\n",
      "Epoch 89, loss : 0.0000, val_loss : 0.0008, acc : 1.000, val_acc : 1.000\n",
      "Epoch 90, loss : 0.0000, val_loss : 0.0008, acc : 1.000, val_acc : 1.000\n",
      "Epoch 91, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 92, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 93, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 94, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 95, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 96, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 97, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 98, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "Epoch 99, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
      "test_acc : 0.900\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlow Task\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "dataset_path =\"data/Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "                          \n",
    "logits = example_net(X)\n",
    "\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n",
    "Iris Species\n",
    "サンプルコードと同じくこの中のtrain.csvを使用してください。目的変数はSpeciesに含まれる3種類全てを使います。\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。\n",
    "それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkhagvadorj/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dataset_path =\"data/Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y[y=='Iris-setosa'] = 2\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y = enc.fit_transform(y[:,np.newaxis])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45258/1704943362.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "/tmp/ipykernel_45258/2084279389.py:56: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 16.3418, val_loss : 38.3685, acc : 0.833, val_acc : 0.500\n",
      "Epoch 1, loss : 0.0000, val_loss : 2.1807, acc : 1.000, val_acc : 0.833\n",
      "Epoch 2, loss : 0.0000, val_loss : 2.2918, acc : 1.000, val_acc : 0.833\n",
      "Epoch 3, loss : 0.0000, val_loss : 1.2339, acc : 1.000, val_acc : 0.875\n",
      "Epoch 4, loss : 0.0000, val_loss : 2.7925, acc : 1.000, val_acc : 0.917\n",
      "Epoch 5, loss : 0.0000, val_loss : 1.0276, acc : 1.000, val_acc : 0.917\n",
      "Epoch 6, loss : 0.0000, val_loss : 0.8947, acc : 1.000, val_acc : 0.875\n",
      "Epoch 7, loss : 0.0000, val_loss : 0.9794, acc : 1.000, val_acc : 0.917\n",
      "Epoch 8, loss : 0.0000, val_loss : 0.7442, acc : 1.000, val_acc : 0.875\n",
      "Epoch 9, loss : 0.0000, val_loss : 1.0601, acc : 1.000, val_acc : 0.917\n",
      "Epoch 10, loss : 0.0000, val_loss : 1.9115, acc : 1.000, val_acc : 0.917\n",
      "Epoch 11, loss : 0.0000, val_loss : 1.8436, acc : 1.000, val_acc : 0.917\n",
      "Epoch 12, loss : 0.0000, val_loss : 2.0354, acc : 1.000, val_acc : 0.917\n",
      "Epoch 13, loss : 0.0000, val_loss : 1.0605, acc : 1.000, val_acc : 0.917\n",
      "Epoch 14, loss : 0.0000, val_loss : 4.6365, acc : 1.000, val_acc : 0.750\n",
      "Epoch 15, loss : 0.0000, val_loss : 4.2473, acc : 1.000, val_acc : 0.750\n",
      "Epoch 16, loss : 0.0000, val_loss : 0.6414, acc : 1.000, val_acc : 0.917\n",
      "Epoch 17, loss : 0.0000, val_loss : 2.4422, acc : 1.000, val_acc : 0.917\n",
      "Epoch 18, loss : 0.0000, val_loss : 4.4404, acc : 1.000, val_acc : 0.875\n",
      "Epoch 19, loss : 0.0000, val_loss : 2.2053, acc : 1.000, val_acc : 0.917\n",
      "Epoch 20, loss : 0.0000, val_loss : 1.0294, acc : 1.000, val_acc : 0.833\n",
      "Epoch 21, loss : 0.0000, val_loss : 1.5306, acc : 1.000, val_acc : 0.917\n",
      "Epoch 22, loss : 0.0000, val_loss : 0.7620, acc : 1.000, val_acc : 0.917\n",
      "Epoch 23, loss : 0.0000, val_loss : 1.8200, acc : 1.000, val_acc : 0.875\n",
      "Epoch 24, loss : 0.0000, val_loss : 0.9383, acc : 1.000, val_acc : 0.958\n",
      "Epoch 25, loss : 0.0000, val_loss : 0.8585, acc : 1.000, val_acc : 0.917\n",
      "Epoch 26, loss : 0.0000, val_loss : 2.8507, acc : 1.000, val_acc : 0.917\n",
      "Epoch 27, loss : 0.0000, val_loss : 1.5511, acc : 1.000, val_acc : 0.917\n",
      "Epoch 28, loss : 0.0000, val_loss : 2.2830, acc : 1.000, val_acc : 0.917\n",
      "Epoch 29, loss : 0.0000, val_loss : 2.8659, acc : 1.000, val_acc : 0.917\n",
      "Epoch 30, loss : 0.0000, val_loss : 2.2119, acc : 1.000, val_acc : 0.917\n",
      "Epoch 31, loss : 0.0000, val_loss : 2.9932, acc : 1.000, val_acc : 0.917\n",
      "Epoch 32, loss : 0.0000, val_loss : 2.6441, acc : 1.000, val_acc : 0.917\n",
      "Epoch 33, loss : 0.0000, val_loss : 2.8842, acc : 1.000, val_acc : 0.917\n",
      "Epoch 34, loss : 0.0000, val_loss : 1.1816, acc : 1.000, val_acc : 0.917\n",
      "Epoch 35, loss : 0.0000, val_loss : 7.3899, acc : 1.000, val_acc : 0.708\n",
      "Epoch 36, loss : 0.0000, val_loss : 1.1266, acc : 1.000, val_acc : 0.958\n",
      "Epoch 37, loss : 0.0000, val_loss : 3.3457, acc : 1.000, val_acc : 0.917\n",
      "Epoch 38, loss : 0.0000, val_loss : 1.9615, acc : 1.000, val_acc : 0.917\n",
      "Epoch 39, loss : 0.0000, val_loss : 1.8206, acc : 1.000, val_acc : 0.917\n",
      "Epoch 40, loss : 0.0000, val_loss : 2.1204, acc : 1.000, val_acc : 0.917\n",
      "Epoch 41, loss : 0.0000, val_loss : 2.5139, acc : 1.000, val_acc : 0.917\n",
      "Epoch 42, loss : 0.0000, val_loss : 0.9945, acc : 1.000, val_acc : 0.917\n",
      "Epoch 43, loss : 0.0000, val_loss : 5.5869, acc : 1.000, val_acc : 0.750\n",
      "Epoch 44, loss : 0.0000, val_loss : 2.1011, acc : 1.000, val_acc : 0.875\n",
      "Epoch 45, loss : 0.0000, val_loss : 1.1464, acc : 1.000, val_acc : 0.917\n",
      "Epoch 46, loss : 0.0000, val_loss : 1.2496, acc : 1.000, val_acc : 0.917\n",
      "Epoch 47, loss : 0.0000, val_loss : 0.9352, acc : 1.000, val_acc : 0.917\n",
      "Epoch 48, loss : 0.0000, val_loss : 1.0428, acc : 1.000, val_acc : 0.958\n",
      "Epoch 49, loss : 0.0000, val_loss : 1.2865, acc : 1.000, val_acc : 0.833\n",
      "Epoch 50, loss : 0.0000, val_loss : 3.6448, acc : 1.000, val_acc : 0.917\n",
      "Epoch 51, loss : 0.0000, val_loss : 1.2872, acc : 1.000, val_acc : 0.875\n",
      "Epoch 52, loss : 0.0000, val_loss : 1.1384, acc : 1.000, val_acc : 0.917\n",
      "Epoch 53, loss : 0.0000, val_loss : 1.2151, acc : 1.000, val_acc : 0.958\n",
      "Epoch 54, loss : 0.0000, val_loss : 1.2076, acc : 1.000, val_acc : 0.917\n",
      "Epoch 55, loss : 0.0000, val_loss : 1.2251, acc : 1.000, val_acc : 0.917\n",
      "Epoch 56, loss : 0.0000, val_loss : 2.1571, acc : 1.000, val_acc : 0.917\n",
      "Epoch 57, loss : 1.3560, val_loss : 11.0006, acc : 0.833, val_acc : 0.750\n",
      "Epoch 58, loss : 0.0000, val_loss : 3.0277, acc : 1.000, val_acc : 0.917\n",
      "Epoch 59, loss : 0.0000, val_loss : 3.7564, acc : 1.000, val_acc : 0.875\n",
      "Epoch 60, loss : 0.0000, val_loss : 1.9007, acc : 1.000, val_acc : 0.917\n",
      "Epoch 61, loss : 0.0000, val_loss : 2.9141, acc : 1.000, val_acc : 0.917\n",
      "Epoch 62, loss : 0.0000, val_loss : 2.8785, acc : 1.000, val_acc : 0.917\n",
      "Epoch 63, loss : 0.0000, val_loss : 1.3439, acc : 1.000, val_acc : 0.958\n",
      "Epoch 64, loss : 0.0000, val_loss : 4.9528, acc : 1.000, val_acc : 0.833\n",
      "Epoch 65, loss : 0.0000, val_loss : 2.6582, acc : 1.000, val_acc : 0.917\n",
      "Epoch 66, loss : 0.0000, val_loss : 5.7981, acc : 1.000, val_acc : 0.917\n",
      "Epoch 67, loss : 0.0000, val_loss : 7.3730, acc : 1.000, val_acc : 0.750\n",
      "Epoch 68, loss : 0.0000, val_loss : 1.4124, acc : 1.000, val_acc : 0.958\n",
      "Epoch 69, loss : 0.0000, val_loss : 4.1873, acc : 1.000, val_acc : 0.917\n",
      "Epoch 70, loss : 0.0000, val_loss : 1.4310, acc : 1.000, val_acc : 0.833\n",
      "Epoch 71, loss : 0.0000, val_loss : 2.0181, acc : 1.000, val_acc : 0.917\n",
      "Epoch 72, loss : 0.0000, val_loss : 3.3171, acc : 1.000, val_acc : 0.917\n",
      "Epoch 73, loss : 0.0000, val_loss : 1.2614, acc : 1.000, val_acc : 0.917\n",
      "Epoch 74, loss : 0.0000, val_loss : 1.2669, acc : 1.000, val_acc : 0.917\n",
      "Epoch 75, loss : 0.0000, val_loss : 2.8138, acc : 1.000, val_acc : 0.917\n",
      "Epoch 76, loss : 0.0000, val_loss : 2.2405, acc : 1.000, val_acc : 0.917\n",
      "Epoch 77, loss : 0.0000, val_loss : 1.7103, acc : 1.000, val_acc : 0.917\n",
      "Epoch 78, loss : 0.0000, val_loss : 3.3204, acc : 1.000, val_acc : 0.875\n",
      "Epoch 79, loss : 0.0000, val_loss : 3.8930, acc : 1.000, val_acc : 0.917\n",
      "Epoch 80, loss : 2.4115, val_loss : 14.4473, acc : 0.833, val_acc : 0.708\n",
      "Epoch 81, loss : 0.0000, val_loss : 4.6184, acc : 1.000, val_acc : 0.917\n",
      "Epoch 82, loss : 0.0000, val_loss : 9.9802, acc : 1.000, val_acc : 0.708\n",
      "Epoch 83, loss : 0.0000, val_loss : 3.4500, acc : 1.000, val_acc : 0.917\n",
      "Epoch 84, loss : 2.9437, val_loss : 15.2382, acc : 0.833, val_acc : 0.750\n",
      "Epoch 85, loss : 0.0000, val_loss : 1.6369, acc : 1.000, val_acc : 0.833\n",
      "Epoch 86, loss : 0.0000, val_loss : 1.3816, acc : 1.000, val_acc : 0.875\n",
      "Epoch 87, loss : 0.0000, val_loss : 5.4032, acc : 1.000, val_acc : 0.917\n",
      "Epoch 88, loss : 0.0000, val_loss : 1.4453, acc : 1.000, val_acc : 0.917\n",
      "Epoch 89, loss : 0.0000, val_loss : 4.9190, acc : 1.000, val_acc : 0.917\n",
      "Epoch 90, loss : 0.0000, val_loss : 1.4441, acc : 1.000, val_acc : 0.917\n",
      "Epoch 91, loss : 0.0000, val_loss : 4.1651, acc : 1.000, val_acc : 0.917\n",
      "Epoch 92, loss : 0.0000, val_loss : 2.1096, acc : 1.000, val_acc : 0.917\n",
      "Epoch 93, loss : 0.0000, val_loss : 3.2031, acc : 1.000, val_acc : 0.917\n",
      "Epoch 94, loss : 0.0000, val_loss : 1.4913, acc : 1.000, val_acc : 0.958\n",
      "Epoch 95, loss : 0.0000, val_loss : 1.3174, acc : 1.000, val_acc : 0.917\n",
      "Epoch 96, loss : 0.0000, val_loss : 5.7360, acc : 1.000, val_acc : 0.917\n",
      "Epoch 97, loss : 0.0000, val_loss : 3.3023, acc : 1.000, val_acc : 0.833\n",
      "Epoch 98, loss : 0.0000, val_loss : 3.0428, acc : 1.000, val_acc : 0.917\n",
      "Epoch 99, loss : 0.0000, val_loss : 6.4662, acc : 1.000, val_acc : 0.917\n",
      "test_acc : 0.933\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "                            \n",
    "logits = example_net(X)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(tf.nn.softmax(logits), 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "House Prices: Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_ori = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_of_row = len(data_ori)\n",
    "data_loss_rate = data_ori.isnull().sum() / max_num_of_row * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:81\n",
      "after:63\n"
     ]
    }
   ],
   "source": [
    "data_drop_5nan_col = data_ori.dropna(axis=1, thresh=max_num_of_row-5)\n",
    "print(\"before:{}\".format(len(data_ori.columns.values)))\n",
    "print(\"after:{}\".format(len(data_drop_5nan_col.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_to_use = data_drop_5nan_col.dropna(axis=0)\n",
    "data_to_use = data_to_use.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_featuer = ['OverallQual',\n",
    "       'GrLivArea',\n",
    "       'GarageCars',\n",
    "       'GarageArea',\n",
    "       'TotalBsmtSF',\n",
    "       '1stFlrSF',\n",
    "       'FullBath',\n",
    "       'TotRmsAbvGrd',\n",
    "       'YearBuilt',\n",
    "       'YearRemodAdd',\n",
    "      ]\n",
    "\n",
    "col_target = ['SalePrice']\n",
    "\n",
    "col = col_featuer + col_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = data_to_use[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[col_featuer].values\n",
    "Y = data[col_target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45258/1704943362.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "/tmp/ipykernel_45258/469766225.py:54: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 37774.9023, val_loss : 42120.8242, acc : 0.909, val_acc : 0.328\n",
      "Epoch 1, loss : 34963.9375, val_loss : 36535.3633, acc : 0.931, val_acc : 0.417\n",
      "Epoch 2, loss : 25081.0469, val_loss : 39177.1562, acc : 0.964, val_acc : 0.334\n",
      "Epoch 3, loss : 24401.3223, val_loss : 38400.3242, acc : 0.968, val_acc : 0.344\n",
      "Epoch 4, loss : 22653.6934, val_loss : 37356.7656, acc : 0.972, val_acc : 0.337\n",
      "Epoch 5, loss : 21786.9590, val_loss : 35426.8477, acc : 0.977, val_acc : 0.355\n",
      "Epoch 6, loss : 29473.1777, val_loss : 33439.9062, acc : 0.958, val_acc : 0.429\n",
      "Epoch 7, loss : 25761.4062, val_loss : 35153.5000, acc : 0.968, val_acc : 0.348\n",
      "Epoch 8, loss : 18827.7559, val_loss : 37107.3438, acc : 0.979, val_acc : 0.304\n",
      "Epoch 9, loss : 21917.6562, val_loss : 33556.1016, acc : 0.976, val_acc : 0.353\n",
      "Epoch 10, loss : 19800.0996, val_loss : 32508.3672, acc : 0.978, val_acc : 0.389\n",
      "Epoch 11, loss : 20359.9746, val_loss : 34103.6562, acc : 0.978, val_acc : 0.375\n",
      "Epoch 12, loss : 24589.2656, val_loss : 31986.9043, acc : 0.970, val_acc : 0.396\n",
      "Epoch 13, loss : 20469.6504, val_loss : 32384.3535, acc : 0.977, val_acc : 0.374\n",
      "Epoch 14, loss : 21706.4629, val_loss : 31079.7734, acc : 0.974, val_acc : 0.372\n",
      "Epoch 15, loss : 16642.1406, val_loss : 31607.0352, acc : 0.979, val_acc : 0.293\n",
      "Epoch 16, loss : 17887.8027, val_loss : 32226.0000, acc : 0.978, val_acc : 0.303\n",
      "Epoch 17, loss : 18935.3125, val_loss : 33058.7500, acc : 0.977, val_acc : 0.270\n",
      "Epoch 18, loss : 17409.7637, val_loss : 30690.4102, acc : 0.976, val_acc : 0.291\n",
      "Epoch 19, loss : 17441.7246, val_loss : 32859.2148, acc : 0.980, val_acc : 0.342\n",
      "Epoch 20, loss : 19753.7188, val_loss : 31665.8262, acc : 0.975, val_acc : 0.281\n",
      "Epoch 21, loss : 17601.1230, val_loss : 30770.0195, acc : 0.975, val_acc : 0.311\n",
      "Epoch 22, loss : 18089.8750, val_loss : 30642.1992, acc : 0.972, val_acc : 0.280\n",
      "Epoch 23, loss : 18352.5469, val_loss : 32183.9707, acc : 0.976, val_acc : 0.317\n",
      "Epoch 24, loss : 20777.9531, val_loss : 31316.2539, acc : 0.975, val_acc : 0.396\n",
      "Epoch 25, loss : 17518.4824, val_loss : 30180.0547, acc : 0.975, val_acc : 0.311\n",
      "Epoch 26, loss : 20477.5566, val_loss : 34062.5312, acc : 0.977, val_acc : 0.310\n",
      "Epoch 27, loss : 20889.9160, val_loss : 32106.1270, acc : 0.976, val_acc : 0.249\n",
      "Epoch 28, loss : 21675.0684, val_loss : 32059.3711, acc : 0.974, val_acc : 0.312\n",
      "Epoch 29, loss : 20531.0156, val_loss : 32034.8086, acc : 0.974, val_acc : 0.247\n",
      "Epoch 30, loss : 22121.6250, val_loss : 32329.2910, acc : 0.974, val_acc : 0.265\n",
      "Epoch 31, loss : 22330.6816, val_loss : 34042.2148, acc : 0.975, val_acc : 0.199\n",
      "Epoch 32, loss : 20927.3184, val_loss : 32430.3223, acc : 0.975, val_acc : 0.295\n",
      "Epoch 33, loss : 18096.7285, val_loss : 31357.0703, acc : 0.975, val_acc : 0.255\n",
      "Epoch 34, loss : 19224.8379, val_loss : 33288.0664, acc : 0.978, val_acc : 0.313\n",
      "Epoch 35, loss : 20478.7168, val_loss : 31189.1836, acc : 0.974, val_acc : 0.240\n",
      "Epoch 36, loss : 18493.9922, val_loss : 32435.4395, acc : 0.977, val_acc : 0.270\n",
      "Epoch 37, loss : 21356.3145, val_loss : 31056.5469, acc : 0.973, val_acc : 0.216\n",
      "Epoch 38, loss : 22410.9473, val_loss : 33547.4258, acc : 0.973, val_acc : 0.234\n",
      "Epoch 39, loss : 19083.5371, val_loss : 33644.5156, acc : 0.977, val_acc : 0.223\n",
      "Epoch 40, loss : 17960.6230, val_loss : 30163.4375, acc : 0.973, val_acc : 0.298\n",
      "Epoch 41, loss : 18985.0332, val_loss : 31466.5430, acc : 0.975, val_acc : 0.345\n",
      "Epoch 42, loss : 23865.5098, val_loss : 34072.9062, acc : 0.972, val_acc : 0.219\n",
      "Epoch 43, loss : 19122.1855, val_loss : 30359.5723, acc : 0.974, val_acc : 0.338\n",
      "Epoch 44, loss : 18411.2676, val_loss : 30270.7598, acc : 0.973, val_acc : 0.281\n",
      "Epoch 45, loss : 19467.2891, val_loss : 29754.7695, acc : 0.971, val_acc : 0.341\n",
      "Epoch 46, loss : 18730.1406, val_loss : 31603.1133, acc : 0.977, val_acc : 0.269\n",
      "Epoch 47, loss : 21153.3066, val_loss : 33960.6211, acc : 0.975, val_acc : 0.274\n",
      "Epoch 48, loss : 24020.7969, val_loss : 33006.4648, acc : 0.972, val_acc : 0.207\n",
      "Epoch 49, loss : 18459.8672, val_loss : 31306.7305, acc : 0.976, val_acc : 0.290\n",
      "Epoch 50, loss : 18857.0645, val_loss : 30151.4922, acc : 0.973, val_acc : 0.269\n",
      "Epoch 51, loss : 20017.1094, val_loss : 31078.0918, acc : 0.976, val_acc : 0.299\n",
      "Epoch 52, loss : 18334.2559, val_loss : 31474.8887, acc : 0.976, val_acc : 0.311\n",
      "Epoch 53, loss : 20612.1406, val_loss : 30591.1816, acc : 0.971, val_acc : 0.238\n",
      "Epoch 54, loss : 26513.8535, val_loss : 33197.8750, acc : 0.965, val_acc : 0.202\n",
      "Epoch 55, loss : 24418.0371, val_loss : 31554.1641, acc : 0.968, val_acc : 0.195\n",
      "Epoch 56, loss : 18946.9980, val_loss : 30231.0176, acc : 0.976, val_acc : 0.333\n",
      "Epoch 57, loss : 21072.7422, val_loss : 30065.8887, acc : 0.973, val_acc : 0.270\n",
      "Epoch 58, loss : 16539.9238, val_loss : 30475.7285, acc : 0.978, val_acc : 0.318\n",
      "Epoch 59, loss : 19248.6621, val_loss : 32088.1582, acc : 0.977, val_acc : 0.329\n",
      "Epoch 60, loss : 17745.3887, val_loss : 30656.5586, acc : 0.977, val_acc : 0.311\n",
      "Epoch 61, loss : 17954.8184, val_loss : 29444.9336, acc : 0.974, val_acc : 0.353\n",
      "Epoch 62, loss : 18020.1719, val_loss : 29766.9961, acc : 0.974, val_acc : 0.310\n",
      "Epoch 63, loss : 20513.5781, val_loss : 30771.0391, acc : 0.976, val_acc : 0.365\n",
      "Epoch 64, loss : 21664.1934, val_loss : 29948.0469, acc : 0.973, val_acc : 0.311\n",
      "Epoch 65, loss : 17522.1973, val_loss : 30358.1348, acc : 0.975, val_acc : 0.306\n",
      "Epoch 66, loss : 17457.1348, val_loss : 29379.5840, acc : 0.970, val_acc : 0.324\n",
      "Epoch 67, loss : 18888.5801, val_loss : 29766.4102, acc : 0.973, val_acc : 0.295\n",
      "Epoch 68, loss : 17390.6855, val_loss : 30726.9043, acc : 0.977, val_acc : 0.304\n",
      "Epoch 69, loss : 17969.5723, val_loss : 30537.0664, acc : 0.976, val_acc : 0.317\n",
      "Epoch 70, loss : 18483.0547, val_loss : 29544.2207, acc : 0.972, val_acc : 0.299\n",
      "Epoch 71, loss : 15929.7861, val_loss : 29439.1895, acc : 0.972, val_acc : 0.329\n",
      "Epoch 72, loss : 17047.4551, val_loss : 31067.9531, acc : 0.977, val_acc : 0.299\n",
      "Epoch 73, loss : 20802.1270, val_loss : 30095.2285, acc : 0.973, val_acc : 0.360\n",
      "Epoch 74, loss : 19850.2715, val_loss : 29961.1055, acc : 0.973, val_acc : 0.299\n",
      "Epoch 75, loss : 19213.7656, val_loss : 29482.6348, acc : 0.975, val_acc : 0.313\n",
      "Epoch 76, loss : 17569.7188, val_loss : 29591.8652, acc : 0.972, val_acc : 0.323\n",
      "Epoch 77, loss : 19014.3223, val_loss : 29472.9297, acc : 0.974, val_acc : 0.372\n",
      "Epoch 78, loss : 19455.5645, val_loss : 29181.6660, acc : 0.968, val_acc : 0.345\n",
      "Epoch 79, loss : 17662.0723, val_loss : 29352.0059, acc : 0.971, val_acc : 0.342\n",
      "Epoch 80, loss : 22566.0137, val_loss : 29498.5488, acc : 0.971, val_acc : 0.417\n",
      "Epoch 81, loss : 18494.6406, val_loss : 29901.6211, acc : 0.973, val_acc : 0.317\n",
      "Epoch 82, loss : 19414.4707, val_loss : 29356.1758, acc : 0.974, val_acc : 0.326\n",
      "Epoch 83, loss : 20125.2656, val_loss : 29374.6172, acc : 0.969, val_acc : 0.377\n",
      "Epoch 84, loss : 17381.4551, val_loss : 29160.1465, acc : 0.970, val_acc : 0.350\n",
      "Epoch 85, loss : 17212.4297, val_loss : 29245.4688, acc : 0.972, val_acc : 0.337\n",
      "Epoch 86, loss : 18259.1094, val_loss : 29018.4551, acc : 0.972, val_acc : 0.341\n",
      "Epoch 87, loss : 20482.1484, val_loss : 29718.9629, acc : 0.973, val_acc : 0.359\n",
      "Epoch 88, loss : 19134.7637, val_loss : 29456.7910, acc : 0.967, val_acc : 0.314\n",
      "Epoch 89, loss : 18731.9668, val_loss : 29712.5371, acc : 0.973, val_acc : 0.308\n",
      "Epoch 90, loss : 23309.3828, val_loss : 31051.8398, acc : 0.969, val_acc : 0.215\n",
      "Epoch 91, loss : 16312.4766, val_loss : 29535.7012, acc : 0.975, val_acc : 0.312\n",
      "Epoch 92, loss : 18995.8887, val_loss : 29005.2949, acc : 0.969, val_acc : 0.320\n",
      "Epoch 93, loss : 19373.7129, val_loss : 29089.6367, acc : 0.973, val_acc : 0.362\n",
      "Epoch 94, loss : 20334.3125, val_loss : 29243.1758, acc : 0.966, val_acc : 0.364\n",
      "Epoch 95, loss : 21232.2598, val_loss : 29236.9766, acc : 0.971, val_acc : 0.357\n",
      "Epoch 96, loss : 17674.0605, val_loss : 29389.7109, acc : 0.977, val_acc : 0.373\n",
      "Epoch 97, loss : 18880.6113, val_loss : 29614.2773, acc : 0.977, val_acc : 0.360\n",
      "Epoch 98, loss : 18723.9219, val_loss : 29859.0469, acc : 0.977, val_acc : 0.312\n",
      "Epoch 99, loss : 19351.7285, val_loss : 28992.9375, acc : 0.971, val_acc : 0.336\n",
      "test_acc : 0.763\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = x_train.shape[1]\n",
    "n_samples = x_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "                            \n",
    "logits = example_net(X)\n",
    "loss_op = tf.reduce_mean(tf.abs(Y - logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "correct_pred = logits\n",
    "\n",
    "accuracy = 1 - (tf.reduce_sum(tf.square(Y - correct_pred)) / tf.reduce_sum(tf.square(Y - tf.reduce_mean(Y))))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: x_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【Task5】MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45258/1522112106.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_train = x_train.astype(np.float)\n",
      "/tmp/ipykernel_45258/1522112106.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_test = x_test.astype(np.float)\n",
      "/home/lkhagvadorj/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 784)\n",
      "(24000, 784)\n"
     ]
    }
   ],
   "source": [
    "#train , val\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.4)\n",
    "print(x_train.shape) \n",
    "print(x_val.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lkhagvadorj/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45258/1704943362.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "/tmp/ipykernel_45258/217344465.py:55: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 2.7374, val_loss : 1.3441, acc : 0.700, val_acc : 0.610\n",
      "Epoch 1, loss : 4.8243, val_loss : 1.1141, acc : 0.900, val_acc : 0.645\n",
      "Epoch 2, loss : 1.4630, val_loss : 0.9842, acc : 0.900, val_acc : 0.725\n",
      "Epoch 3, loss : 1.2008, val_loss : 0.9319, acc : 0.900, val_acc : 0.768\n",
      "Epoch 4, loss : 0.7674, val_loss : 0.6764, acc : 0.800, val_acc : 0.835\n",
      "Epoch 5, loss : 0.4747, val_loss : 0.4681, acc : 0.800, val_acc : 0.886\n",
      "Epoch 6, loss : 0.2550, val_loss : 0.4071, acc : 0.900, val_acc : 0.901\n",
      "Epoch 7, loss : 0.4328, val_loss : 0.4041, acc : 0.900, val_acc : 0.903\n",
      "Epoch 8, loss : 0.8776, val_loss : 0.4288, acc : 0.800, val_acc : 0.903\n",
      "Epoch 9, loss : 0.6516, val_loss : 0.3854, acc : 0.800, val_acc : 0.916\n",
      "Epoch 10, loss : 0.6349, val_loss : 0.4547, acc : 0.800, val_acc : 0.891\n",
      "Epoch 11, loss : 0.6388, val_loss : 0.3800, acc : 0.800, val_acc : 0.923\n",
      "Epoch 12, loss : 0.8099, val_loss : 0.3753, acc : 0.900, val_acc : 0.921\n",
      "Epoch 13, loss : 0.5199, val_loss : 0.3887, acc : 0.900, val_acc : 0.919\n",
      "Epoch 14, loss : 0.4069, val_loss : 0.3371, acc : 0.900, val_acc : 0.930\n",
      "Epoch 15, loss : 0.4380, val_loss : 0.3571, acc : 0.900, val_acc : 0.925\n",
      "Epoch 16, loss : 0.3895, val_loss : 0.4416, acc : 0.900, val_acc : 0.917\n",
      "Epoch 17, loss : 0.3708, val_loss : 0.3707, acc : 0.900, val_acc : 0.925\n",
      "Epoch 18, loss : 0.3154, val_loss : 0.3456, acc : 0.900, val_acc : 0.929\n",
      "Epoch 19, loss : 0.3507, val_loss : 0.4433, acc : 0.900, val_acc : 0.925\n",
      "Epoch 20, loss : 0.4450, val_loss : 0.3740, acc : 0.800, val_acc : 0.928\n",
      "Epoch 21, loss : 0.3233, val_loss : 0.4478, acc : 0.900, val_acc : 0.927\n",
      "Epoch 22, loss : 0.0397, val_loss : 0.3945, acc : 1.000, val_acc : 0.933\n",
      "Epoch 23, loss : 0.0770, val_loss : 0.3588, acc : 1.000, val_acc : 0.932\n",
      "Epoch 24, loss : 0.3240, val_loss : 0.4133, acc : 0.900, val_acc : 0.927\n",
      "Epoch 25, loss : 0.3157, val_loss : 0.4653, acc : 0.900, val_acc : 0.924\n",
      "Epoch 26, loss : 0.2864, val_loss : 0.3976, acc : 0.900, val_acc : 0.934\n",
      "Epoch 27, loss : 0.0205, val_loss : 0.4637, acc : 1.000, val_acc : 0.932\n",
      "Epoch 28, loss : 0.5081, val_loss : 0.4250, acc : 0.800, val_acc : 0.933\n",
      "Epoch 29, loss : 0.4176, val_loss : 0.4849, acc : 0.800, val_acc : 0.929\n",
      "Epoch 30, loss : 0.3261, val_loss : 0.5004, acc : 0.900, val_acc : 0.933\n",
      "Epoch 31, loss : 0.3593, val_loss : 0.4346, acc : 0.900, val_acc : 0.928\n",
      "Epoch 32, loss : 0.3766, val_loss : 0.4314, acc : 0.900, val_acc : 0.934\n",
      "Epoch 33, loss : 0.2747, val_loss : 0.5342, acc : 0.900, val_acc : 0.932\n",
      "Epoch 34, loss : 0.3383, val_loss : 0.5323, acc : 0.900, val_acc : 0.929\n",
      "Epoch 35, loss : 0.3373, val_loss : 0.4920, acc : 0.900, val_acc : 0.933\n",
      "Epoch 36, loss : 0.3888, val_loss : 0.4902, acc : 0.900, val_acc : 0.932\n",
      "Epoch 37, loss : 0.3510, val_loss : 0.6467, acc : 0.900, val_acc : 0.924\n",
      "Epoch 38, loss : 0.3187, val_loss : 0.5207, acc : 0.900, val_acc : 0.929\n",
      "Epoch 39, loss : 0.2904, val_loss : 0.4810, acc : 0.900, val_acc : 0.932\n",
      "Epoch 40, loss : 0.3887, val_loss : 0.5190, acc : 0.900, val_acc : 0.923\n",
      "Epoch 41, loss : 0.3317, val_loss : 0.5700, acc : 0.900, val_acc : 0.935\n",
      "Epoch 42, loss : 0.3601, val_loss : 0.4676, acc : 0.900, val_acc : 0.927\n",
      "Epoch 43, loss : 0.3398, val_loss : 0.5434, acc : 0.900, val_acc : 0.930\n",
      "Epoch 44, loss : 0.2894, val_loss : 0.5775, acc : 0.900, val_acc : 0.929\n",
      "Epoch 45, loss : 0.2826, val_loss : 0.6447, acc : 0.900, val_acc : 0.930\n",
      "Epoch 46, loss : 0.3163, val_loss : 0.5641, acc : 0.900, val_acc : 0.933\n",
      "Epoch 47, loss : 0.3217, val_loss : 0.5596, acc : 0.900, val_acc : 0.930\n",
      "Epoch 48, loss : 0.3196, val_loss : 0.5950, acc : 0.900, val_acc : 0.932\n",
      "Epoch 49, loss : 0.3575, val_loss : 0.5650, acc : 0.900, val_acc : 0.931\n",
      "Epoch 50, loss : 0.2809, val_loss : 0.5139, acc : 0.900, val_acc : 0.928\n",
      "Epoch 51, loss : 0.2865, val_loss : 0.5977, acc : 0.900, val_acc : 0.934\n",
      "Epoch 52, loss : 0.2899, val_loss : 0.7107, acc : 0.900, val_acc : 0.926\n",
      "Epoch 53, loss : 0.3259, val_loss : 0.6042, acc : 0.900, val_acc : 0.933\n",
      "Epoch 54, loss : 0.0610, val_loss : 0.7598, acc : 1.000, val_acc : 0.931\n",
      "Epoch 55, loss : 0.0376, val_loss : 0.5725, acc : 1.000, val_acc : 0.936\n",
      "Epoch 56, loss : 0.0338, val_loss : 0.6727, acc : 1.000, val_acc : 0.928\n",
      "Epoch 57, loss : 0.3631, val_loss : 0.6227, acc : 0.900, val_acc : 0.931\n",
      "Epoch 58, loss : 0.3587, val_loss : 0.5900, acc : 0.900, val_acc : 0.933\n",
      "Epoch 59, loss : 0.3585, val_loss : 0.6202, acc : 0.900, val_acc : 0.927\n",
      "Epoch 60, loss : 0.3163, val_loss : 0.6794, acc : 0.900, val_acc : 0.932\n",
      "Epoch 61, loss : 0.3321, val_loss : 0.6738, acc : 0.900, val_acc : 0.927\n",
      "Epoch 62, loss : 0.9912, val_loss : 0.6721, acc : 0.900, val_acc : 0.925\n",
      "Epoch 63, loss : 0.0192, val_loss : 0.6607, acc : 1.000, val_acc : 0.908\n",
      "Epoch 64, loss : 0.4169, val_loss : 0.9306, acc : 0.900, val_acc : 0.923\n",
      "Epoch 65, loss : 0.2910, val_loss : 0.7053, acc : 0.900, val_acc : 0.930\n",
      "Epoch 66, loss : 0.2949, val_loss : 0.7659, acc : 0.900, val_acc : 0.929\n",
      "Epoch 67, loss : 0.0114, val_loss : 0.8570, acc : 1.000, val_acc : 0.924\n",
      "Epoch 68, loss : 0.2526, val_loss : 0.6827, acc : 0.900, val_acc : 0.929\n",
      "Epoch 69, loss : 0.0262, val_loss : 0.6965, acc : 1.000, val_acc : 0.920\n",
      "Epoch 70, loss : 0.2789, val_loss : 0.8424, acc : 0.900, val_acc : 0.932\n",
      "Epoch 71, loss : 0.3261, val_loss : 0.7898, acc : 0.900, val_acc : 0.922\n",
      "Epoch 72, loss : 0.0096, val_loss : 0.8197, acc : 1.000, val_acc : 0.930\n",
      "Epoch 73, loss : 0.3008, val_loss : 0.7301, acc : 0.900, val_acc : 0.931\n",
      "Epoch 74, loss : 0.3644, val_loss : 0.7804, acc : 0.900, val_acc : 0.934\n",
      "Epoch 75, loss : 0.3047, val_loss : 0.8522, acc : 0.800, val_acc : 0.921\n",
      "Epoch 76, loss : 0.0617, val_loss : 0.8919, acc : 1.000, val_acc : 0.924\n",
      "Epoch 77, loss : 0.0212, val_loss : 0.9261, acc : 1.000, val_acc : 0.927\n",
      "Epoch 78, loss : 0.0335, val_loss : 0.9827, acc : 1.000, val_acc : 0.922\n",
      "Epoch 79, loss : 0.3050, val_loss : 0.8070, acc : 0.900, val_acc : 0.926\n",
      "Epoch 80, loss : 0.3625, val_loss : 1.1234, acc : 0.900, val_acc : 0.926\n",
      "Epoch 81, loss : 0.3702, val_loss : 0.9526, acc : 0.900, val_acc : 0.928\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = x_train.shape[1]\n",
    "n_samples = x_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                             \n",
    "logits = example_net(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(tf.nn.softmax(logits), 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: x_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: x_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
