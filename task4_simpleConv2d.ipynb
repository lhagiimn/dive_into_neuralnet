{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(\"int\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "    \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "            \n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC2:\n",
    "\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.W_feedback = 0\n",
    "        self.B_feedback = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        self.W_feedback = self.dW / self.dA.shape[0]\n",
    "        self.B_feedback = np.average(self.dA, axis=0)\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma = 0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    " \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "       \n",
    "        layer.B = layer.B - self.lr * layer.B_feedback    \n",
    "        layer.W = layer.W - self.lr * layer.W_feedback\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return (1 - self._func(X)) * self._func(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "    \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "           \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "  \n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return 1 - (self._func(X))**2\n",
    "        \n",
    "    def forward(self, X):\n",
    "    \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "        self.pred = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        #X = X - np.max(X)\n",
    "        #tmp = np.exp(X)\n",
    "        #denominator = np.sum(tmp, axis=1)\n",
    "        #output = tmp / denominator[:, np.newaxis]\n",
    "        tmp = X - np.max(X)\n",
    "        output = np.exp(tmp) / np.sum(np.exp(tmp))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return X\n",
    "        \n",
    "    def forward(self, X):\n",
    "          \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        self.pred = A\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "   \n",
    "        dZ = self.pred - dA\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "   \n",
    "    def __init__(self):\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return np.where( X > 0, 1, 0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "      \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "           \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) / np.sqrt(self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    " \n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) * np.sqrt(2 / self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "  \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.H_B = 1\n",
    "        self.H_W = 1\n",
    "    def update(self, layer):\n",
    "            \n",
    "        #dA, dWを更新＆保存\n",
    "        self.H_B = self.H_B + np.average(layer.dA)**2\n",
    "        self.H_W = self.H_W + np.average(layer.dW)**2\n",
    "        \n",
    "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0) / np.sqrt(self.H_B)\n",
    "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0] / np.sqrt(self.H_W)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Conv2d():\n",
    "  \n",
    "    def __init__(self, n_input_hight, n_input_width, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.n_input_hight = n_input_hight\n",
    "        self.n_input_width = n_input_width\n",
    "        self.W = f_w    #(n_output, n_ch, f_size_h, f_size_w)\n",
    "        self.B = f_b    #(1, n_ch, n_output)\n",
    "        self.n_output = self.W.shape[0]\n",
    "        self.n_input_ch = self.W.shape[1]\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.f_width = f_w.shape[3]\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.n_output_width = self.n_input_width - self.f_width + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "           \n",
    "        self.input_X_forward = X\n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        A = np.zeros((batch_size, self.n_output, self.n_input_ch, self.n_output_hight, self.n_output_width))\n",
    "        B = self.B[0]\n",
    "        B = B.T\n",
    "        B = B[np.newaxis]\n",
    "        X = X[:,np.newaxis]\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            for w in range(self.n_output_width):\n",
    "                w1 = w\n",
    "                w2 = w + self.f_width\n",
    "                X_seg = X[:,:,:,h1:h2,w1:w2]\n",
    "                tmp = np.sum(np.sum(X_seg * self.W, axis=4), axis=3)\n",
    "                tmp = tmp + B\n",
    "                A[:,:,:,h,w] = tmp\n",
    "\n",
    "        output = np.sum(A, axis=2)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "            \n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[1] ,1 ,1))\n",
    "        dL = np.zeros((batch_size, X.shape[1], dA.shape[2], dA.shape[3]))\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            tmp = dA[:,i][:,np.newaxis]\n",
    "            dL[:,o1:o2] = np.tile(tmp, (self.n_input_ch,1 ,1))\n",
    "\n",
    "        loop1 = self.n_input_hight - self.n_output_hight + 1\n",
    "        loop2 = self.n_input_width - self.n_output_width + 1\n",
    "        dW_tmp = np.zeros((batch_size, X.shape[1], loop1, loop2))\n",
    "        for h in range(loop1):\n",
    "            h1 = h\n",
    "            h2 = h + self.n_output_hight\n",
    "            for w in range(loop2):\n",
    "                w1 = w\n",
    "                w2 = w + self.n_output_width\n",
    "                dX_seg = X[:,:, h1:h2, w1:w2]\n",
    "                dW_tmp[:,:,h,w] = np.sum(np.sum(dL * dX_seg, axis=3), axis=2)\n",
    "        \n",
    "        dW_tmp2 = np.average(dW_tmp, axis=0)     \n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            self.W_feedback[i] = dW_tmp2[o1:o2]\n",
    "\n",
    "        dB = np.sum(np.sum(dA, axis=3), axis=2)\n",
    "        dB = np.average(dB, axis=0) \n",
    "        for i in range(self.n_input_ch):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(self.n_output):\n",
    "            dA_tmp = dA[:,i][:,np.newaxis,:]\n",
    "            dA_padding = np.zeros([batch_size, 1, self.f_hight-1, dA_tmp.shape[3]])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=2)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=2) \n",
    "            \n",
    "            dA_padding = np.zeros([batch_size, 1, dA_tmp.shape[2], self.f_width-1])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=3)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=3) \n",
    "            dA_tmp = np.tile(dA_tmp, (self.n_input_ch ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                for w in range(self.n_input_width):\n",
    "                    w1 = w\n",
    "                    w2 = w + self.f_width\n",
    "                    \n",
    "                    dA_seg = dA_tmp[:,:,h1:h2, w1:w2]\n",
    "          \n",
    "                    dA_seg = np.fliplr(np.fliplr(dA_seg).T).T\n",
    "                    tmp = np.sum(np.sum(dA_seg * self.W[i], axis=3), axis=2)\n",
    "                    dZ_seg[:,:,h,w] = tmp\n",
    "                \n",
    "            self.Z_feedback += dZ_seg\n",
    "            \n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(0.01)\n",
    "initializer = XavierInitializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "w = np.ones([2,2,2,2])\n",
    "b = np.ones([1,2,2])\n",
    "A = np.random.randint(0,10,(1,2,6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Cov = Conv2d(6,8,w,b,initializer,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dA = Cov.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0, 6, 3, 1, 7, 8, 3, 7],\n",
       "         [6, 7, 9, 6, 9, 6, 7, 8],\n",
       "         [5, 6, 3, 7, 2, 7, 8, 2],\n",
       "         [3, 1, 8, 1, 0, 0, 5, 1],\n",
       "         [5, 0, 3, 8, 0, 8, 9, 4],\n",
       "         [6, 9, 0, 1, 7, 2, 6, 3]],\n",
       "\n",
       "        [[4, 4, 5, 0, 0, 8, 6, 6],\n",
       "         [4, 8, 9, 0, 7, 5, 6, 3],\n",
       "         [9, 0, 4, 2, 1, 7, 1, 0],\n",
       "         [2, 7, 8, 8, 0, 6, 5, 4],\n",
       "         [7, 4, 0, 6, 4, 6, 4, 0],\n",
       "         [2, 8, 7, 8, 6, 4, 4, 9]]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[41., 53., 35., 32., 52., 51., 48.],\n",
       "         [47., 48., 42., 36., 46., 49., 37.],\n",
       "         [35., 39., 43., 23., 25., 41., 28.],\n",
       "         [31., 33., 44., 29., 26., 45., 34.],\n",
       "         [43., 33., 35., 42., 39., 45., 41.]],\n",
       "\n",
       "        [[41., 53., 35., 32., 52., 51., 48.],\n",
       "         [47., 48., 42., 36., 46., 49., 37.],\n",
       "         [35., 39., 43., 23., 25., 41., 28.],\n",
       "         [31., 33., 44., 29., 26., 45., 34.],\n",
       "         [43., 33., 35., 42., 39., 45., 41.]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 82, 188, 176, 134, 168, 206, 198,  96],\n",
       "         [176, 378, 356, 290, 332, 396, 370, 170],\n",
       "         [164, 338, 344, 288, 260, 322, 310, 130],\n",
       "         [132, 276, 318, 278, 206, 274, 296, 124],\n",
       "         [148, 280, 290, 300, 272, 310, 330, 150],\n",
       "         [ 86, 152, 136, 154, 162, 168, 172,  82]],\n",
       "\n",
       "        [[ 82, 188, 176, 134, 168, 206, 198,  96],\n",
       "         [176, 378, 356, 290, 332, 396, 370, 170],\n",
       "         [164, 338, 344, 288, 260, 322, 310, 130],\n",
       "         [132, 276, 318, 278, 206, 274, 296, 124],\n",
       "         [148, 280, 290, 300, 272, 310, 330, 150],\n",
       "         [ 86, 152, 136, 154, 162, 168, 172,  82]]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cov.backward(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_output_data_size(input_X, filter_W, padding_size_h, padding_size_w, stride_h, stride_w):\n",
    "    input_x_hight = input_X.shape[1]\n",
    "    input_x_width = input_X.shape[2]\n",
    "    output_ch = filter_W[0]\n",
    "    filter_w_hight = filter_W.shape[3]\n",
    "    filter_w_width = filter_W.shape[4]\n",
    "    \n",
    "    out_h = (input_x_hight + padding_size_h * 2 - filter_W_hight) / stride_h + 1\n",
    "    out_w = (input_x_width + padding_size_w * 2 - filter_W_width) / stride_w + 1\n",
    "    \n",
    "    return out_ch, out_h, out_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Max_pooling():\n",
    "    \n",
    "    def __init__(self, stride_h, stride_w):\n",
    "        self.h = stride_h\n",
    "        self.w = stride_w\n",
    "        self.max_pos = 0\n",
    "        self.backward_map = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, ch, h, w)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        ch_size = X.shape[1]\n",
    "        h_size = X.shape[2]\n",
    "        w_size = X.shape[3]\n",
    "        \n",
    "        output_size_h = (int)(h_size / self.h) \n",
    "        output_size_w = (int)(w_size / self.w)\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        self.backward_map = np.zeros((batch_size, ch_size, output_size_h, output_size_w, self.h, self.w))\n",
    "        \n",
    "       \n",
    "        for n_h in range(output_size_h):\n",
    "            for n_w in range(output_size_w):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w\n",
    "\n",
    "                tmp = np.max(np.max(X[:,:, pos_h1:pos_h2, pos_w1:pos_w2], axis=3), axis=2)\n",
    "                output[:,:, n_h, n_w] = tmp\n",
    "                tmp = tmp[:,:,np.newaxis,np.newaxis]\n",
    "                self.backward_map[:,:, n_h, n_w] = (X[:,:, pos_h1:pos_h2, pos_w1:pos_w2] == tmp)\n",
    "                \n",
    "      \n",
    "        self.backward_map = self.backward_map.astype(int)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        dA.shape (batch_size, ch, h, w)\n",
    "        \"\"\"        \n",
    "        batch_size = dA.shape[0]\n",
    "        ch_size = dA.shape[1]\n",
    "        h_size = dA.shape[2]\n",
    "        w_size = dA.shape[3]\n",
    "        \n",
    "        output_size_h = h_size * self.h\n",
    "        output_size_w = w_size * self.w\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        for n_h in range(h_size):\n",
    "            for n_w in range(w_size):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w                    \n",
    "                \n",
    "                tmp = dA[:,:, n_h, n_w][:,:, np.newaxis, np.newaxis]\n",
    "                output[:,:, pos_h1:pos_h2, pos_w1:pos_w2] = tmp * self.backward_map[:,:, n_h, n_w]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Max_pooling(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.random.randint(0,10,(1,3,6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dA = p.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " [[[[8 4 5 3 4 2 2 7]\n",
      "   [9 5 2 1 4 4 1 8]\n",
      "   [9 2 5 8 2 9 4 9]\n",
      "   [6 1 2 6 2 0 0 6]\n",
      "   [9 6 2 3 2 0 9 6]\n",
      "   [1 4 4 2 5 2 3 2]]\n",
      "\n",
      "  [[3 8 2 4 7 3 3 3]\n",
      "   [6 2 5 8 7 3 3 7]\n",
      "   [2 2 0 7 0 9 5 4]\n",
      "   [8 6 4 1 6 5 8 7]\n",
      "   [4 2 2 9 0 1 4 9]\n",
      "   [4 3 0 5 8 8 7 2]]\n",
      "\n",
      "  [[5 1 5 2 1 2 9 6]\n",
      "   [7 4 6 1 5 7 5 4]\n",
      "   [8 2 9 6 1 8 7 9]\n",
      "   [1 1 1 9 7 9 8 9]\n",
      "   [3 5 1 1 0 7 6 2]\n",
      "   [6 4 1 0 4 4 7 0]]]]\n",
      "output:\n",
      " [[[[9. 5. 4. 8.]\n",
      "   [9. 8. 9. 9.]\n",
      "   [9. 4. 5. 9.]]\n",
      "\n",
      "  [[8. 8. 7. 7.]\n",
      "   [8. 7. 9. 8.]\n",
      "   [4. 9. 8. 9.]]\n",
      "\n",
      "  [[7. 6. 7. 9.]\n",
      "   [8. 9. 9. 9.]\n",
      "   [6. 1. 7. 7.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"input:\\n\",A)\n",
    "print(\"output:\\n\",dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 5., 0., 4., 0., 0., 0.],\n",
       "         [9., 0., 0., 0., 4., 4., 0., 8.],\n",
       "         [9., 0., 0., 8., 0., 9., 0., 9.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [9., 0., 0., 0., 0., 0., 9., 0.],\n",
       "         [0., 0., 4., 0., 5., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 8., 0., 0., 7., 0., 0., 0.],\n",
       "         [0., 0., 0., 8., 7., 0., 0., 7.],\n",
       "         [0., 0., 0., 7., 0., 9., 0., 0.],\n",
       "         [8., 0., 0., 0., 0., 0., 8., 0.],\n",
       "         [4., 0., 0., 9., 0., 0., 0., 9.],\n",
       "         [4., 0., 0., 0., 8., 8., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 9., 0.],\n",
       "         [7., 0., 6., 0., 0., 7., 0., 0.],\n",
       "         [8., 0., 9., 0., 0., 0., 0., 9.],\n",
       "         [0., 0., 0., 9., 0., 9., 0., 9.],\n",
       "         [0., 0., 1., 1., 0., 7., 0., 0.],\n",
       "         [6., 0., 1., 0., 0., 0., 7., 0.]]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.backward(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten2():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_X_shape = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, n_output, n_feature1, n_feature2)\n",
    "        \n",
    "        return (batch_size, n_output * n_feature1 * n_feature2)\n",
    "        \"\"\"\n",
    "        self.inout_X_shape = X.shape\n",
    "        output = X.reshape([self.inout_X_shape[0], self.inout_X_shape[1] * self.inout_X_shape[2] * self.inout_X_shape[3]])\n",
    "        return output\n",
    "    \n",
    "    def backward(self, X):\n",
    "        output = X.reshape(self.inout_X_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2():\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.loss = 0\n",
    "        self.loss_val = 0\n",
    "        self.activation_func = 0\n",
    "        self.affine_func = 0\n",
    "        self.n_layer = 0\n",
    "        self.layer_instance = [0 for _ in range(64)]\n",
    "    \n",
    "        \n",
    "    def _crossentropy(self, y_pred, y):\n",
    "        INF_AVOIDANCE = 1e-8\n",
    "        cross_entropy = -1 * y * np.log(y_pred + INF_AVOIDANCE)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def add_layer(self, model):\n",
    "        self.layer_instance[self.n_layer] = model\n",
    "        self.n_layer += 1\n",
    "        return\n",
    "    \n",
    "    def delet_all_layer(self):\n",
    "        self.layer_instance[0:self.n_layer] = 0\n",
    "        self.n_layer = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        self.loss_val = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            #print(\"Proceeding Epoch:\", i+1)\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                for layer in range(self.n_layer):\n",
    "                    X = self.layer_instance[layer].forward(X)\n",
    "                    #print(\"layer:{} X:\\n{}\".format(layer, X))\n",
    "                \n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                    \n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    dz = self.layer_instance[layer].backward(dz)\n",
    "\n",
    "                \n",
    "                loop_count += 1\n",
    "                \n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self._predict(X_val)\n",
    "                self.loss_val[i] = self._crossentropy(y_val_pred, y_val)\n",
    "                \n",
    "            if self.verbose:\n",
    "                print(\"Epoch:{} \\n Loss:\\n{} Loss(val):\\n{}\".format(i+1, self.loss[i], self.loss_val[i]))\n",
    "                \n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        for layer in range(self.n_layer):\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "\n",
    "    def _predict(self, X):\n",
    "        for layer in range(self.n_layer):\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d():\n",
    "\n",
    "    def __init__(self, n_input_hight, n_input_width, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_input_hight = n_input_hight\n",
    "        self.n_input_width = n_input_width\n",
    "        self.W = f_w    #(n_output, n_ch, f_size_h, f_size_w)\n",
    "        self.B = f_b    #(1, n_ch, n_output)\n",
    "        self.n_output = self.W.shape[0]\n",
    "        self.n_input_ch = self.W.shape[1]\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.f_width = f_w.shape[3]\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.n_output_width = self.n_input_width - self.f_width + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "            \n",
    "        self.input_X_forward = X\n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        A = np.zeros((batch_size, self.n_output, self.n_input_ch, self.n_output_hight, self.n_output_width))\n",
    "        B = self.B[0]\n",
    "        B = B.T\n",
    "        B = B[np.newaxis]\n",
    "        X = X[:,np.newaxis]\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            for w in range(self.n_output_width):\n",
    "                w1 = w\n",
    "                w2 = w + self.f_width\n",
    "                X_seg = X[:,:,:,h1:h2,w1:w2]\n",
    "                tmp = np.sum(np.sum(X_seg * self.W, axis=4), axis=3)\n",
    "                tmp = tmp + B\n",
    "                A[:,:,:,h,w] = tmp\n",
    "\n",
    "        output = np.sum(A, axis=2)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "       \n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[1] ,1 ,1))\n",
    "        dL = np.zeros((batch_size, X.shape[1], dA.shape[2], dA.shape[3]))\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            tmp = dA[:,i][:,np.newaxis]\n",
    "            dL[:,o1:o2] = np.tile(tmp, (self.n_input_ch,1 ,1))\n",
    "        \n",
    "        loop1 = self.n_input_hight - self.n_output_hight + 1\n",
    "        loop2 = self.n_input_width - self.n_output_width + 1\n",
    "        dW_tmp = np.zeros((batch_size, X.shape[1], loop1, loop2))\n",
    "        for h in range(loop1):\n",
    "            h1 = h\n",
    "            h2 = h + self.n_output_hight\n",
    "            for w in range(loop2):\n",
    "                w1 = w\n",
    "                w2 = w + self.n_output_width\n",
    "                dX_seg = X[:,:, h1:h2, w1:w2]\n",
    "                dW_tmp[:,:,h,w] = np.sum(np.sum(dL * dX_seg, axis=3), axis=2)\n",
    "        \n",
    "        dW_tmp2 = np.average(dW_tmp, axis=0)     \n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            self.W_feedback[i] = dW_tmp2[o1:o2]\n",
    "\n",
    "        dB = np.sum(np.sum(dA, axis=3), axis=2)\n",
    "        dB = np.average(dB, axis=0) \n",
    "        for i in range(self.n_input_ch):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(self.n_output):\n",
    "            dA_tmp = dA[:,i][:,np.newaxis,:]\n",
    "            dA_padding = np.zeros([batch_size, 1, self.f_hight-1, dA_tmp.shape[3]])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=2)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=2) \n",
    "            \n",
    "            dA_padding = np.zeros([batch_size, 1, dA_tmp.shape[2], self.f_width-1])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=3)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=3) \n",
    "            dA_tmp = np.tile(dA_tmp, (self.n_input_ch ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                for w in range(self.n_input_width):\n",
    "                    w1 = w\n",
    "                    w2 = w + self.f_width\n",
    "                    \n",
    "                    dA_seg = dA_tmp[:,:,h1:h2, w1:w2]\n",
    "                    dA_seg = np.fliplr(np.fliplr(dA_seg).T).T\n",
    "                    tmp = np.sum(np.sum(dA_seg * self.W[i], axis=3), axis=2)\n",
    "                    dZ_seg[:,:,h,w] = tmp\n",
    "                \n",
    "            self.Z_feedback += dZ_seg\n",
    "\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1, 28, 28)\n",
      "(57000, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17078/692121211.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_train = x_train.astype(np.float)\n",
      "/tmp/ipykernel_17078/692121211.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_test = x_test.astype(np.float)\n",
      "/home/lkhagvadorj/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "\n",
    "x_train = x_train[:,np.newaxis,:]\n",
    "x_test = x_test[:,np.newaxis,:]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.95)\n",
    "print(x_train.shape) # (48000, 784)\n",
    "print(x_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "w = np.random.randn(2,1,2,2)\n",
    "b = np.random.randn(1,1,2)\n",
    "#A = np.random.randint(0,10,(1,2,6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN2 = ScratchDeepNeuralNetrowkClassifier2(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN2.add_layer(Conv2d(x_train.shape[2],x_train.shape[3], w, b, initializer, optimizer))\n",
    "CNN2.add_layer(Flatten2())\n",
    "CNN2.add_layer(FC2(w.shape[0] * (x_train.shape[2] - w.shape[2] + 1) * (x_train.shape[3] - w.shape[3] + 1), 100, initializer, optimizer))\n",
    "CNN2.add_layer(Sigmoid())\n",
    "CNN2.add_layer(FC2(100, 10, initializer, optimizer))\n",
    "CNN2.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17078/595005678.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "CNN2.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CNN2.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Pred=\\n\", y_pred)\n",
    "print(\"Yval=\\n\", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss = np.array(CNN2.loss)\n",
    "loss_ave = np.average(loss, axis=1)\n",
    "\n",
    "loss_val = np.array(CNN2.loss_val)\n",
    "loss_val_ave = np.average(loss_val, axis=1)\n",
    "\n",
    "plt.title(\"epoch vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ave, \"r\", label=\"loss\")\n",
    "plt.plot(loss_val_ave, \"b\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "n_batch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN3 = ScratchDeepNeuralNetrowkClassifier2(n_epoch, n_batch, verbose =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "conv_w_lenet1 = np.random.randn(6,1,5,5)\n",
    "conv_b_lenet1 = np.random.randn(1,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_w_lenet2 = np.random.randn(16,6,5,5)\n",
    "conv_b_lenet2 = np.random.randn(1,6,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN3.add_layer(Conv2d(x_train.shape[2],x_train.shape[3], conv_w_lenet1, conv_b_lenet1, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(ReLU())\n",
    "CNN3.add_layer(Max_pooling(2,2))\n",
    "CNN3.add_layer(Conv2d(12,12, conv_w_lenet2, conv_b_lenet2, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(ReLU())\n",
    "CNN3.add_layer(Max_pooling(2,2))\n",
    "CNN3.add_layer(Flatten2())\n",
    "CNN3.add_layer(FC2(256, 120, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(ReLU())\n",
    "CNN3.add_layer(FC2(120, 84, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(ReLU())\n",
    "CNN3.add_layer(FC2(84, 10, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNN3.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = CNN3.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss = np.array(CNN3.loss)\n",
    "loss_ave = np.average(loss, axis=1)\n",
    "\n",
    "loss_val = np.array(CNN3.loss_val)\n",
    "loss_val_ave = np.average(loss_val, axis=1)\n",
    "\n",
    "plt.title(\"epoch vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ave, \"r\", label=\"loss\")\n",
    "plt.plot(loss_val_ave, \"b\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max_pooling():\n",
    "    \n",
    "    def __init__(self, stride_h, stride_w):\n",
    "        self.h = stride_h\n",
    "        self.w = stride_w\n",
    "        self.max_pos = 0\n",
    "        self.backward_map = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, ch, h, w)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        ch_size = X.shape[1]\n",
    "        h_size = X.shape[2]\n",
    "        w_size = X.shape[3]\n",
    "        \n",
    "        output_size_h = (int)(h_size / self.h) \n",
    "        output_size_w = (int)(w_size / self.w)\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        self.backward_map = np.zeros((batch_size, ch_size, output_size_h, output_size_w, self.h, self.w))\n",
    "        \n",
    "        for n_h in range(output_size_h):\n",
    "            for n_w in range(output_size_w):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w\n",
    "\n",
    "                tmp = np.average(np.average(X[:,:, pos_h1:pos_h2, pos_w1:pos_w2], axis=3), axis=2)\n",
    "                output[:,:, n_h, n_w] = tmp\n",
    "                tmp = tmp[:,:,np.newaxis,np.newaxis]\n",
    "                self.backward_map[:,:, n_h, n_w] = (X[:,:, pos_h1:pos_h2, pos_w1:pos_w2] == tmp)\n",
    "                \n",
    "        self.backward_map = self.backward_map.astype(int)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        dA.shape (batch_size, ch, h, w)\n",
    "        \"\"\"        \n",
    "        batch_size = dA.shape[0]\n",
    "        ch_size = dA.shape[1]\n",
    "        h_size = dA.shape[2]\n",
    "        w_size = dA.shape[3]\n",
    "        \n",
    "        output_size_h = h_size * self.h\n",
    "        output_size_w = w_size * self.w\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        for n_h in range(h_size):\n",
    "            for n_w in range(w_size):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w                    \n",
    "                   \n",
    "                tmp = dA[:,:, n_h, n_w][:,:, np.newaxis, np.newaxis]\n",
    "                output[:,:, pos_h1:pos_h2, pos_w1:pos_w2] = tmp * self.backward_map[:,:, n_h, n_w]\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
